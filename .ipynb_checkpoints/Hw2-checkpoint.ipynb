{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H 2.1 Logistic regression: Suppose we have labeled data(xi,yi), where **x**<sub>i</sub>∈Rd,y∈{0,1}. We want to compute **w** that minimizes the following negative log-likelihood function:  \n",
    "\n",
    "$$\\text{minimize}_w\\sum y_{i}log(1+exp(-w^Tx_{i})) + (1-y_i)log(1+exp(w^Tx_i))$$\n",
    "\n",
    "Let us denote the objective of this function as J(w). (Note that we could also define y_i∈{−1,1} and rewrite the objective in (1) as J(w) = $\\sum log(1+exp(-y_iw^Tx_{i}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Show the the following log-logistic function is convex:  \n",
    "$$f(w) = log(1+exp(-w^Tx))$$  \n",
    "\n",
    "You may assume that __x__ is a scalar and show that the second derivative of $f(\\mathbf{w})$ w.r.t. __w__ is always positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{gather*}\n",
    "\\frac{\\partial f(w)}{\\partial w} = \\frac{\\partial}{\\partial w} log(1 + e^{(-w^Tx)}) \\\\\n",
    "\\frac{\\partial f(w)}{\\partial w} = \\frac{e^{(-w^Tx)}*-x}{1+e^{(-w^Tx)}} \\\\\n",
    "\\\\\n",
    "\\frac{\\partial^2 f(w)}{\\partial w} = \\frac{\\partial}{\\partial w}\\frac{e^{(-w^Tx)}*-x}{1+e^{(-w^Tx)}} \\\\\n",
    "\\frac{\\partial^2 f(w)}{\\partial w} = -x[(x)(e^{(-w^Tx)})^2(1+e^{(-w^Tx)})^{-2} + (-x)e^{(-w^Tx)}(1+e^{-w^Tx})^{-1}] \\\\\n",
    "\\frac{\\partial^2 f(w)}{\\partial w} = x^2\\left[-\\left(\\frac{e^{(-w^Tx)}}{1+e^{(-w^Tx)}}\\right)^2 + \\frac{e^{(-w^Tx)}}{1+e^{(-w^Tx)}}\\right] \\\\\n",
    "\\frac{\\partial^2 f(w)}{\\partial w} = x^2\\left[\\frac{e^{(-w^Tx)} + e^{(-2w^Tx)} - e^{(-2w^Tx)}}{(1+e^{(-w^Tx)})^2} \\right] \\\\\n",
    "\\frac{\\partial^2 f(w)}{\\partial w} = x^2 \\left[ \\frac{e^{(-w^Tx)}}{(1+e^{(-w^Tx)})^2} \\right] \\\\\n",
    "\\text{Both terms are always greater than 0. Therefore, } \\\\\n",
    "\\frac{\\partial^2 f(w)}{\\partial w} \\geq 0\n",
    "\\end{gather*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Next you will write a Python script for logistic regression that solves (1) using gradient descent algorithm with a fixed step size α. You will then use your code to learn to classify images of digits from the MNIST dataset.  \n",
    "\n",
    "You can show that the expression for $\\nabla_w J = - \\sum_{i=1}^N (y_i−h_w(x_i))x_i$, where $h(w) = \\frac{1}{1+e^{(-w^Tx)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "mnist =sio.loadmat('mnistoriginal.mat')\n",
    "data=mnist['data']\n",
    "label=mnist['label']\n",
    "data = (data - np.mean(data,axis=0))/(np.std(data,axis=0)+0.1)\n",
    "\n",
    "train_x = data[:,0:60000]\n",
    "train_y = label[0,0:60000]\n",
    "test_x = data[:,60000:70000]\n",
    "test_y = label[0,60000:70000]\n",
    "\n",
    "# Choose only two digits\n",
    "class_0=0\n",
    "class_1=1\n",
    "idx_train=[]\n",
    "for i in range (0,train_y.shape[0]):\n",
    "    if (train_y[i]==class_0) or (train_y[i]==class_1):\n",
    "        idx_train=np.append(idx_train,i)\n",
    "        \n",
    "idx_test=[]\n",
    "for i in range (0,test_y.shape[0]):\n",
    "    if (test_y[i]==class_0) or (test_y[i]==class_1):\n",
    "        idx_test=np.append(idx_test,i)\n",
    "                \n",
    "train_x=np.transpose(train_x)\n",
    "test_x=np.transpose(test_x)\n",
    "trainx=[]\n",
    "trainy=[]\n",
    "testx=[]\n",
    "testy=[]\n",
    "\n",
    "for i in range(0,idx_train.shape[0]):\n",
    "    trainx.append(train_x[np.int(idx_train[i]),:])\n",
    "    if train_y[np.int(idx_train[i])]==class_0:\n",
    "        trainy.append(0)\n",
    "    else:\n",
    "        trainy.append(1)\n",
    "        \n",
    "for i in range(0,idx_test.shape[0]):\n",
    "    testx.append(test_x[np.int(idx_test[i]),:])\n",
    "    if test_y[np.int(idx_test[i])]==class_0:\n",
    "        testy.append(0)\n",
    "    else:\n",
    "        testy.append(1)\n",
    "        \n",
    "train_x=np.array(trainx)\n",
    "train_y = np.array(trainy)\n",
    "test_x = np.array(testx)\n",
    "test_y = np.array(testy)\n",
    "\n",
    "train_x = np.insert(train_x,0,1,axis=1)\n",
    "test_x = np.insert(test_x,0,1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "sigmoid_vec = np.vectorize(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(w, x, y):\n",
    "    h = sigmoid_vec(np.matmul(x,w))\n",
    "    return -(np.matmul((y - h),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(w, x, y):\n",
    "    a = 0.000001\n",
    "    if(np.all(gradient(w,x,y) < 0)):\n",
    "        return logistic_regression(w - a*gradient(w,x,y),x,y)\n",
    "    else:\n",
    "        return w - a*gradient(w,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.6903073286052\n"
     ]
    }
   ],
   "source": [
    "w = logistic_regression(np.ones(train_x.shape[1]),train_x, train_y)\n",
    "#np.matmul(train_x, np.zeros(train_x.shape[1])).shape\n",
    "predict = np.round(sigmoid_vec(np.matmul(test_x.astype(float),w)))\n",
    "acc =100.0*np.sum(test_y == predict)/test_y.shape[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H 2.2 Perceptron learning algorithm (PLA) and kernel extension: In this problem you will implement PLA to build a simple binary classification system. Suppose we have labeled data $(\\mathbf{x}_i, y_i)$, where $\\mathbf{x}_i ∈ R^{d+1}$ with $\\mathbf{x}_i(1) = 1$ and y ∈ {-1, +1}. Let us define $h_{\\mathbf{w}}(\\mathbf{x}_i) = sign(\\mathbf{w}^T\\mathbf{x}_i)$. We want to computer **w** using PLA.  <br><br>\n",
    "(a) Data Processing: Generate two examples of 2D *linearly separable* dataset with N = 100 samples each. (To do this, you will first generate a weight vector and constant term,w, and then assign ±1 labels to your data samples as $y_i=h_{\\mathbf{w}}(\\mathbf{x}_i)$.) Let us call the two datasets “Data1” and“Data2”. For Data1, randomly select 80% of the samples for training and the remaining 20% for testing on Data1 (80/20). For Data2, randomly select 30% of the samples for training and the remaining 70% for testing (30/70)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "w = np.random.rand(2)\n",
    "\n",
    "data1 = np.random.randn(N,2)\n",
    "data2 = np.random.randn(N,2)\n",
    "\n",
    "y1 = np.sign(np.matmul(data1,w))\n",
    "y2 = np.sign(np.matmul(data2,w))\n",
    "\n",
    "y1_idx = np.random.choice(100, 100,replace=False)\n",
    "y1_train = y1[y1_idx[:80]]\n",
    "y1_test = y1[y1_idx[81:]]\n",
    "\n",
    "y2_idx = np.random.choice(100,100, replace=False)\n",
    "y2_train = y2[y2_idx[:30]]\n",
    "y2_test = y2[y2_idx[31:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
